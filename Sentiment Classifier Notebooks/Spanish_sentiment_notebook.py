# -*- coding: utf-8 -*-
"""XLMRoberta for sentiment analysis (Spanish).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MnNGwewJl7rDg1gx6WhQ_pub--sTJ0Dy
"""

# This sentiment analysis program is modeled off the BERT tutorial by Chris McCormick and Nick Ryan found here: 
# https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX

# Citation: 
# Chris McCormick and Nick Ryan. (2019, July 22). BERT Fine-Tuning Tutorial with PyTorch. Retrieved from http://www.mccormickml.com

# For questions regarding fair usage of this notebook, please contact the secondary author at [EMAIL REMOVED] and the source coauthor at chris.mccormick@nearist.ai

import torch

# If there's a GPU available...
if torch.cuda.is_available():    

    # Tell PyTorch to use the GPU, and specify which one to use (if applicable)    
    device = torch.device("cuda")

    print('There are %d GPU(s) available.' % torch.cuda.device_count())

    print('We will use the GPU:', torch.cuda.get_device_name(0))

# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

import numpy as np # data structure library
import math # handy for operations we'll be using
import time # used in running a timer during our training phase
import re # used for searching text and identifying patterns therein
from bs4 import BeautifulSoup # used in reading data from text files and cleaning it up
import random
import pandas as pd

# The library we'll be using to import and build our model (XLMRobertaForSequenceClassification)
!pip install transformers

# Reads in our training tweets (hydrated from tweet IDs found here: https://www.clarin.si/repository/xmlui/handle/11356/1054)
train_data = pd.read_json("Data/spanish_tweets_twarc_NEW.json")

train_data.to_csv(path_or_buf='Data/spanish_tweets_twarc_NEW.csv') # Converts the above file to a CSV

features = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30", "31"] # The hydrated tweet IDs come loaded with a ton of information, most of it useless for our purposes—thus the purely numerical column names.
train_data = pd.read_csv("Data/spanish_tweets_twarc_NEW.csv", index_col=False, header=None, names=features, engine='python', encoding='utf-8') # Note the encoding: utf-8 works well with European languages like Spanish

# Loads in a file containing the hand-labeled tweet sentiments and IDs (Note: the first imported file contains tweets and their IDs; this one contains tweet sentiments and their IDs. Our goal now is to match tweets with their sentiments).
features = ["Tweet ID", "sentiment", "_"]
sentiment_and_ids = pd.read_csv("Data/Spanish_Twitter_sentiment.csv", index_col=False, header=None, names=features, engine='python', encoding='utf-8')

# features = ["1", "sentiment", "3"]
# sentiments = pd.read_csv("Data/Spanish_Twitter_sentiment.csv", header=None, names=features, engine='python', encoding='utf-8')

sentiments = sentiment_and_ids["sentiment"][1:] # Gives us a dataframe with just the sentiments

hydrated_tweets = train_data['5'][1:] # Gives us a dataframe with just the tweets

print(len(sentiment_and_ids)) # Should be 275589

tweet_ids = sentiment_and_ids['Tweet ID'][1:] # Gives us a dataframe with just the IDs

hydrated_tweet_ids = train_data['3'][1:] # Gives us a dataframe with just the IDs of the tweets that were successfully hydrated

tweet_ids = list(tweet_ids)
hydrated_tweet_ids = list(hydrated_tweet_ids)

# Unfortunately, all of the data we imported is quite messy and needs to be thoroughly cleaned. This nasty-looking function
# goes through and collects the indices of duplicates in the data files so we can remove them. And no, not all of the duplicates were 
# sequential; I tried writing a much nicer linear-time function, but it left a few stragglers :( Thus this gross piece of work.

from collections import Counter # Used to construct multiset to find duplicates
def get_duplicate_indices(glist):
    c = Counter(glist)
    dups = []
    for item in c:
        if c[item] > 1:
            dups.append(item)
    print(len(dups))
    more_dup_inds = []
    k = 0
    for dup in dups:
        clone_inds = []
        for i in range(len(glist)):
            if glist[i] == dup:
                clone_inds.append(i)
        more_dup_inds.append(clone_inds)
        print(k) # Helps keep track of where the algorithm is in the cleaning process. Also helps maintain your sanity while waiting for this goliath to finish.
        k += 1
    for clone_inds in more_dup_inds:
        clone_inds.remove(min(clone_inds))
    new_duplicates = []
    for clone_inds in more_dup_inds:
        for clone in clone_inds:
            new_duplicates.append(clone)
    return new_duplicates

new_duplicate_ids = get_duplicate_indices(tweet_ids) # Finds duplicates in tweet IDs

duplicate_indices = new_duplicate_ids

duplicate_indices_3 = get_duplicate_indices(hydrated_tweet_ids) # Finds duplicates in hydrated tweet IDs

duplicate_indices_2 = list(dict.fromkeys(duplicate_indices_3))

# Creates a list of unique tweet IDs

new_tweet_ids = []

for i in range(len(tweet_ids)):
    if i not in duplicate_indices:
        new_tweet_ids.append(tweet_ids[i])

# Creates lists for the training labels and hydrated tweet IDs containing unique data points

new_sentiment_list = []
new_hydrated_tweet_ids= []
for j in range(1, len(sentiments)+1):
    if j-1 not in duplicate_indices:
        new_sentiment_list.append(sentiments[j])

for k in range(len(hydrated_tweet_ids)):
    if k not in duplicate_indices_2:
        new_hydrated_tweet_ids.append(hydrated_tweet_ids[k])

tweet_ids = new_tweet_ids
hydrated_tweet_ids = new_hydrated_tweet_ids
sentiments = new_sentiment_list

# Another problem with our data is that many of the tweets whose IDs were contained in the original dataset failed to be hydrated (using two different hydration programs, Hydrator and twarc).
# This cell collects up all the IDs of tweets that failed to be hydrated
missing_indices = []
for i in range(len(tweet_ids)):
    print(i)
    if tweet_ids[i] not in hydrated_tweet_ids:
        missing_indices.append(i)

# Gives us only the sentiments of tweets that were hydrated
more_new_sentiments = []
for i in range(len(sentiments)):
    if i not in missing_indices:
        more_new_sentiments.append(sentiments[i])

# One last problem with the data was that there was some random noise in the list of hydrated tweet IDs. 
# If something in the ID dataset isn't the proper length, throw it out.
for tweet_id in hydrated_tweet_ids:
    if len(str(tweet_id)) != 18: # length of each ID
        hydrated_tweet_ids.remove(tweet_id)

# Purposeful duplicate, because the last function leaves one ID remaining for whatever reason
for tweet_id in hydrated_tweet_ids:
    if len(str(tweet_id)) != 18:
        hydrated_tweet_ids.remove(tweet_id)

print(len(hydrated_tweet_ids)) # Should be 50907

sentiments = more_new_sentiments
tweet_ids = hydrated_tweet_ids

print(len(sentiments)==len(tweet_ids)) # Should print True.

# You can use this to check that the tweet IDs and sentiments are finally aligned
print(tweet_ids[len(tweet_ids)-10:len(tweet_ids)])
print(sentiments[len(tweet_ids)-10:len(tweet_ids)])

# Creates a dictionary with IDs as keys and sentiments as values
id_sentiment_dict = {}
for tweet_id in tweet_ids:
    index = list(train_data['3']).index(tweet_id)
    id_sentiment_dict[tweet_id] = train_data['5'][index]

train_tweets = [id_sentiment_dict[tweet_id] for tweet_id in id_sentiment_dict] # Gives us just the tweets we want, after throwing out all the unusable data

tweets = train_tweets

# gets the tweet into the format we want
def clean_tweet(tweet):
  tweet = BeautifulSoup(tweet, "lxml").get_text() # turns xml-formatted text into regular text
  tweet = re.sub(r"@[A-Za-z0-9]+", " ", tweet) # gets rid of all user references in tweets (i.e. "@username")
  tweet = re.sub(r"https?://[A-Za-z0-9./]+", " ", tweet) # gets rid of URLs
  tweet = re.sub(r"[^A-Za-z.!?áéíóúüñ¿ÁÉÍÓÚÜÑ']", " ", tweet) # gets rid of any non-standard characters in the tweets
  tweet = re.sub(r" +", " ", tweet) # replaces all excess whitespace with a single space

  return tweet # gives us our cleaned tweet

clean_train_data = [clean_tweet(tweets[i]) for i in range(len(tweets))] # Gives us our cleaned tweets

tweets = clean_train_data

training_data_labels = list(sentiments) # fetches the sentiment values from our training dataset 

for i in range(len(training_data_labels)):
    if training_data_labels[i] == 'Positive':
        training_data_labels[i] = 1
    if training_data_labels[i] == "Negative":
        training_data_labels[i] = 0

# Throws out all the neutral sentiments in our dataset

neutral_indices = []
new_training_data_labels = []
for i in range(len(training_data_labels)):
    if training_data_labels[i] == 'Neutral':
        neutral_indices.append(i)

for j in range(len(training_data_labels)):
    if j not in neutral_indices:
        new_training_data_labels.append(training_data_labels[j])

training_data_labels = new_training_data_labels

print(len(training_data_labels)) # Should be 29334
print(set(training_data_labels)) # Should be {0, 1}

# Throws out all the neutral-sentiments tweets in our dataset
new_tweets = []
for i in range(len(tweets)):
    if i not in neutral_indices:
        new_tweets.append(tweets[i])

tweets = new_tweets

print(len(tweets)==len(training_data_labels))) # Should print True

sentiments = training_data_labels

# Gives us the tally of positive- and negative-sentiment tweets in our dataset. You can even them out if you'd like, but
# we actually got decent results without doing so.

count0 = 0
count1 = 1
for label in training_data_labels:
    if label == 0:
        count0 += 1
    if label == 1:
        count1 += 1

print(count0, count1)

# # Use the following commented-out cells to even out the number of positive- and negative-sentiment tweets

# extra_pos_indices = []
# i = 0
# for j in range(len(training_data_labels)):
#     if training_data_labels[j] == 1 and i < (count1 - count0):
#         extra_pos_indices.append(j)
#         i += 1
#     elif i >= (count1 - count0):
#         break

# print(len(extra_pos_indices))

# new_training_data_labels = []
# for i in range(len(training_data_labels)):
#     if i not in extra_pos_indices:
#         new_training_data_labels.append(training_data_labels[i])

# training_data_labels = new_training_data_labels

# new_tweets = []
# for i in range(len(tweets)):
#     if i not in extra_pos_indices:
#         new_tweets.append(tweets[i])

# tweets = new_tweets

# count0 = 0
# count1 = 1
# for label in training_data_labels:
#     if label == 0:
#         count0 += 1
#     if label == 1:
#         count1 += 1

# print(count0, count1) # Should be the exact same

# We're going to use the XLMRobertaTokenizer. Documentation: https://huggingface.co/transformers/model_doc/xlmroberta.html
from transformers import XLMRobertaTokenizer

tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')

# CODE AND COMMENTS IN THIS CELL ARE ATTRIBUTABLE TO CHRIS MCCORMICK AND NICK RYAN, NOT THE SECONDARY AUTHOR ([NAME REMOVED])

max_len = 0

# For every sentence...
for tweet in tweets:

    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.
    input_ids = tokenizer.encode(tweet, add_special_tokens=True)

    # Update the maximum sentence length.
    max_len = max(max_len, len(input_ids))

print('Max sentence length: ', max_len)

# CODE AND COMMENTS IN THIS CELL ARE ATTRIBUTABLE TO CHRIS MCCORMICK AND NICK RYAN, NOT THE SECONDARY AUTHOR ([NAME REMOVED])

# Tokenize all of the sentences and map the tokens to their word IDs.
input_ids = []
attention_masks = []

# For every sentence...
for tweet in tweets:
    # `encode_plus` will:
    #   (1) Tokenize the sentence.
    #   (2) Prepend the `[CLS]` token to the start.
    #   (3) Append the `[SEP]` token to the end.
    #   (4) Map tokens to their IDs.
    #   (5) Pad or truncate the sentence to `max_length`
    #   (6) Create attention masks for [PAD] tokens.
    encoded_dict = tokenizer.encode_plus(
                        tweet,                      # Sentence to encode.
                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                        max_length = 64,           # Pad & truncate all sentences.
                        truncation=True,           # Explicitly enable truncation
                        pad_to_max_length = True,
                        return_attention_mask = True,   # Construct attn. masks.
                        return_tensors = 'pt',     # Return pytorch tensors.
                   )
    
    # Add the encoded sentence to the list.    
    input_ids.append(encoded_dict['input_ids'])
    
    # And its attention mask (simply differentiates padding from non-padding).
    attention_masks.append(encoded_dict['attention_mask'])

# Convert the lists into tensors.
input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
sentiments = torch.tensor(training_data_labels)

# Print sentence 0, now as a list of IDs.
print('Original: ', tweets[0])
print('Token IDs:', input_ids[0])

# print(input_ids.shape)
# print(attention_masks.shape)
# print(sentiments.shape)

# CODE AND COMMENTS IN THIS CELL ARE ATTRIBUTABLE TO CHRIS MCCORMICK AND NICK RYAN, NOT THE SECONDARY AUTHOR ([NAME REMOVED])

from torch.utils.data import TensorDataset, random_split

# Combine the training inputs into a TensorDataset.
dataset = TensorDataset(input_ids, attention_masks, sentiments)

# Create a 90-10 train-validation split.

# Calculate the number of samples to include in each set.
train_size = int(0.95 * len(dataset))
val_size = len(dataset) - train_size

# Divide the dataset by randomly selecting samples.
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

print('{:>5,} training samples'.format(train_size))
print('{:>5,} validation samples'.format(val_size))

# CODE AND COMMENTS IN THIS CELL ARE ATTRIBUTABLE TO CHRIS MCCORMICK AND NICK RYAN, NOT THE SECONDARY AUTHOR ([NAME REMOVED])

from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

# The DataLoader needs to know our batch size for training, so we specify it 
# here. For fine-tuning BERT on a specific task, the authors recommend a batch 
# size of 16 or 32.
batch_size = 32

# Create the DataLoaders for our training and validation sets.
# We'll take training samples in random order. 
train_dataloader = DataLoader(
            train_dataset,  # The training samples.
            shuffle=True, # Shuffle the data
            batch_size = batch_size # Trains with this batch size.
        )

# For validation the order doesn't matter, so we'll just read them sequentially.
validation_dataloader = DataLoader(
            val_dataset, # The validation samples.
            shuffle=True, # Shuffle the data
            batch_size = batch_size) # Evaluate with this batch size.

device = torch.device('cuda') # Make sure we're using the GPU

# MOST CODE AND COMMENTS IN THIS CELL ARE ATTRIBUTABLE TO CHRIS MCCORMICK AND NICK RYAN, NOT THE SECONDARY AUTHOR ([NAME REMOVED])

from transformers import XLMRobertaForSequenceClassification, AdamW, BertConfig

# Load BertForSequenceClassification, the pretrained BERT model with a single 
# linear classification layer on top. 
model = XLMRobertaForSequenceClassification.from_pretrained(
    "xlm-roberta-large", # Use the large XLM Roberta model.
    num_labels = 2, # The number of output labels--2 for binary classification, 3 for ternary, etc.
                    # You can increase this for multi-class tasks.   
    output_attentions = False, # Whether the model returns attentions weights.
    output_hidden_states = False, # Whether the model returns all hidden-states.
)

# Tell pytorch to run this model on the GPU.
model.cuda()

optimizer = AdamW(model.parameters(),
                  lr = 2e-5
                  eps = 1e-8
                )

# MOST CODE AND COMMENTS IN THIS CELL ARE ATTRIBUTABLE TO CHRIS MCCORMICK AND NICK RYAN, NOT THE SECONDARY AUTHOR ([NAME REMOVED])

from transformers import get_cosine_schedule_with_warmup # Note choice of schedule. This was chosen through simple experimentation, but you can play around with this.

# Number of training epochs
epochs = 1

# Total number of training steps is [number of batches] x [number of epochs]. 
# (Note that this is not the same as the number of training samples).
total_steps = len(train_dataloader) * epochs

# Create the learning rate scheduler.
scheduler = get_cosine_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 100, # Default value in run_glue.py
                                            num_training_steps = total_steps)

# Used to compute the training/validation/test accuracies
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

import datetime # Just used for keeping track of elapsed time during training

def format_time(elapsed):
    '''
    Takes a time in seconds and returns a string hh:mm:ss
    '''
    # Round to the nearest second.
    elapsed_rounded = int(round((elapsed)))
    
    # Format as hh:mm:ss
    return str(datetime.timedelta(seconds=elapsed_rounded))

# CODE AND COMMENTS IN THIS CELL ARE ATTRIBUTABLE TO CHRIS MCCORMICK AND NICK RYAN, NOT THE SECONDARY AUTHOR ([NAME REMOVED])

import random

seed_val = 42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

# We'll store a number of quantities such as training and validation loss, 
# validation accuracy, and timings.
training_stats = []

# Measure the total training time for the whole run.
total_t0 = time.time()

# For each epoch...
for epoch_i in range(0, epochs):
    
    # ========================================
    #               Training
    # ========================================
    
    # Perform one full pass over the training set.

    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    print('Training...')

    # Measure how long the training epoch takes.
    t0 = time.time()

    # Reset the total loss for this epoch.
    total_train_loss = 0

    # Put the model into training mode. Don't be misled--the call to 
    # `train` just changes the *mode*, it doesn't *perform* the training.
    # `dropout` and `batchnorm` layers behave differently during training
    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)
    model.train()

    # For each batch of training data...
    for step, batch in enumerate(train_dataloader):

        # Progress update every 40 batches.
        if step % 40 == 0 and not step == 0:
            # Calculate elapsed time in minutes.
            elapsed = format_time(time.time() - t0)
            
            # Report progress.
            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))

        # Unpack this training batch from our dataloader. 
        #
        # As we unpack the batch, we'll also copy each tensor to the GPU using the 
        # `to` method.
        #
        # `batch` contains three pytorch tensors:
        #   [0]: input ids 
        #   [1]: attention masks
        #   [2]: labels 
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)

        # Always clear any previously calculated gradients before performing a
        # backward pass. PyTorch doesn't do this automatically because 
        # accumulating the gradients is "convenient while training RNNs". 
        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)
        model.zero_grad()        

        # Perform a forward pass (evaluate the model on this training batch).
        # The documentation for this `model` function is here: 
        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification
        # It returns different numbers of parameters depending on what arguments
        # arge given and what flags are set. For our useage here, it returns
        # the loss (because we provided labels) and the "logits"--the model
        # outputs prior to activation.
        loss, logits = model(b_input_ids, 
                             token_type_ids=None, 
                             attention_mask=b_input_mask, 
                             labels=b_labels)

        # Accumulate the training loss over all of the batches so that we can
        # calculate the average loss at the end. `loss` is a Tensor containing a
        # single value; the `.item()` function just returns the Python value 
        # from the tensor.
        total_train_loss += loss.item()
        
        
        if step != 0:
            print("Train loss:", total_train_loss / step)


        # Perform a backward pass to calculate the gradients.
        loss.backward()

        # Clip the norm of the gradients to 1.0.
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # Update parameters and take a step using the computed gradient.
        # The optimizer dictates the "update rule"--how the parameters are
        # modified based on their gradients, the learning rate, etc.
        optimizer.step()

        # Update the learning rate.
        scheduler.step()

    # Calculate the average loss over all of the batches.
    avg_train_loss = total_train_loss / len(train_dataloader)     
    
    # Measure how long this epoch took.
    training_time = format_time(time.time() - t0)

    print("")
    print("  Average training loss: {0:.2f}".format(avg_train_loss))
    print("  Training epcoh took: {:}".format(training_time))
        
    # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    print("")
    print("Running Validation...")

    t0 = time.time()

    # Put the model in evaluation mode--the dropout layers behave differently
    # during evaluation.
    model.eval()

    # Tracking variables 
    total_eval_accuracy = 0
    total_eval_loss = 0
    nb_eval_steps = 0

    # Evaluate data for one epoch
    for batch in validation_dataloader:
        
        # Unpack this training batch from our dataloader. 
        #
        # As we unpack the batch, we'll also copy each tensor to the GPU using 
        # the `to` method.
        #
        # `batch` contains three pytorch tensors:
        #   [0]: input ids 
        #   [1]: attention masks
        #   [2]: labels 
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)
        
        # Tell pytorch not to bother with constructing the compute graph during
        # the forward pass, since this is only needed for backprop (training).
        with torch.no_grad():        

            # Forward pass, calculate logit predictions.
            # token_type_ids is the same as the "segment ids", which 
            # differentiates sentence 1 and 2 in 2-sentence tasks.
            # The documentation for this `model` function is here: 
            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification
            # Get the "logits" output by the model. The "logits" are the output
            # values prior to applying an activation function like the softmax.
            (loss, logits) = model(b_input_ids, 
                                   token_type_ids=None, 
                                   attention_mask=b_input_mask,
                                   labels=b_labels)
            
        # Accumulate the validation loss.
        total_eval_loss += loss.item()

        # Move logits and labels to CPU
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences, and
        # accumulate it over all batches.
        total_eval_accuracy += flat_accuracy(logits, label_ids)
        
    # Report the final accuracy for this validation run.
    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)
    print("  Accuracy: {0:.2f}".format(avg_val_accuracy))

    # Calculate the average loss over all of the batches.
    avg_val_loss = total_eval_loss / len(validation_dataloader)
    
    # Measure how long the validation run took.
    validation_time = format_time(time.time() - t0)
    
    print("  Validation Loss: {0:.2f}".format(avg_val_loss))
    print("  Validation took: {:}".format(validation_time))


    # Record all statistics from this epoch.
    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Valid. Loss': avg_val_loss,
            'Valid. Accur.': avg_val_accuracy,
            'Training Time': training_time,
            'Validation Time': validation_time
        }
    )

print("")
print("Training complete!")

print("Total training took {:} (h:mm:ss)".format(format_time(time.time()-total_t0)))

# CODE AND COMMENTS IN THIS CELL ARE ATTRIBUTABLE TO CHRIS MCCORMICK AND NICK RYAN, NOT THE SECONDARY AUTHOR ([NAME REMOVED])

# Display floats with two decimal places.
pd.set_option('precision', 2)

# Create a DataFrame from our training statistics.
df_stats = pd.DataFrame(data=training_stats)

# Use the 'epoch' \as the row index.
df_stats = df_stats.set_index('epoch')

# A hack to force the column headers to wrap.
#df = df.style.set_table_styles([dict(selector="th",props=[('max-width', '70px')])])

# Display the table.
df_stats

# The model stops improving significantly after one epoch

# Reading in our sentiment-labeled test tweets
# Source: https://www.kaggle.com/c/spanish-arilines-tweets-sentiment-analysis/overview/description

features = ["sentiment", "text"]
test_data = pd.read_csv("Data/tweets_public_EDITED.csv", header=None, names=features, engine='python', encoding='UTF-8')

print('Number of tweets in dataset: {:,}\n'.format(test_data.shape[0]))

clean_test_data = [clean_tweet(test_data['text'][i]) for i in range(test_data.shape[0])] # Cleans our test data

test_tweets = clean_test_data

test_data_labels = test_data.sentiment.values # Fetches the sentiment values from our training dataset 
test_data_labels[test_data_labels == 'positive'] = 1
test_data_labels[test_data_labels == 'negative'] = 0

# Throws out neutral sentiments

neutral_indices = []
new_test_data_labels = []
for i in range(len(test_data_labels)):
    if test_data_labels[i] == 'neutral':
        neutral_indices.append(i)

for j in range(len(test_data_labels)):
    if j not in neutral_indices:
        new_test_data_labels.append(test_data_labels[j])

test_data_labels = new_test_data_labels

test_sentiments = test_data_labels

# Throws out neutral tweets
new_test_tweets = []
for i in range(len(test_tweets)):
    if i not in neutral_indices:
        new_test_tweets.append(test_tweets[i])

test_tweets = new_test_tweets

print(set(test_sentiments)) # Should print {0, 1}
print(len(test_sentiments)) # Should print 5258
print(len(test_tweets)) # Should also print 5258

count0 = 0
count1 = 0
for sent in test_sentiments:
    if sent == 0:
        count0 += 1
    if sent == 1:
        count1 += 1

print(count0, count1) # Count of positive and negative tweets will be uneven

# This time, we are going to even out the number of positive and negative tweets to ensure confidence in our test accuracy measure
# First, we remove the sentiment that is in surplus (negative)
extra_neg_indices = []
i = 0
for j in range(len(test_sentiments)):
    if test_sentiments[j] == 0 and i < (count0 - count1):
        extra_neg_indices.append(j)
        i += 1
    elif i >= (count0 - count1):
        break

# We produce lists of sentiment labels and tweets with an even balance of positive and negative sentiments

new_test_sentiments = []
for i in range(len(test_sentiments)):
    if i not in extra_neg_indices:
        new_test_sentiments.append(test_sentiments[i])

test_sentiments = new_test_sentiments

new_test_tweets = []
for i in range(len(test_tweets)):
    if i not in extra_neg_indices:
        new_test_tweets.append(test_tweets[i])

test_tweets = new_test_tweets

print(len(test_tweets))
print(len(test_sentiments))

count0 = 0
count1 = 0
for sent in test_sentiments:
    if sent == 0:
        count0 += 1
    if sent == 1:
        count1 += 1

print(count0==count1) # Now, this should print True

# CODE AND COMMENTS IN THIS CELL ARE ATTRIBUTABLE TO CHRIS MCCORMICK AND NICK RYAN, NOT THE SECONDARY AUTHOR ([NAME REMOVED])

max_len = 0

# For every sentence...
for tweet in test_tweets:

    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.
    test_input_ids = tokenizer.encode(tweet, add_special_tokens=True)

    # Update the maximum sentence length.
    max_len = max(max_len, len(test_input_ids))

print('Max sentence length: ', max_len)

test_sentiments = list(test_sentiments)

# CODE AND COMMENTS IN THIS CELL ARE ATTRIBUTABLE TO CHRIS MCCORMICK AND NICK RYAN, NOT THE SECONDARY AUTHOR ([NAME REMOVED])

# Tokenize all of the sentences and map the tokens to their word IDs.
test_input_ids = []
test_attention_masks = []

# For every sentence...
for tweet in test_tweets:
    # `encode_plus` will:
    #   (1) Tokenize the sentence.
    #   (2) Prepend the `[CLS]` token to the start.
    #   (3) Append the `[SEP]` token to the end.
    #   (4) Map tokens to their IDs.
    #   (5) Pad or truncate the sentence to `max_length`
    #   (6) Create attention masks for [PAD] tokens.
    encoded_dict = tokenizer.encode_plus(
                        tweet,                      # Sentence to encode.
                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                        max_length = 64,           # Pad & truncate all sentences.
                        truncation=True,           # Explicitly enable truncation
                        pad_to_max_length = True,
                        return_attention_mask = True,   # Construct attn. masks.
                        return_tensors = 'pt',     # Return pytorch tensors.
                   )
    
    # Add the encoded sentence to the list.    
    test_input_ids.append(encoded_dict['input_ids'])
    
    # And its attention mask (simply differentiates padding from non-padding).
    test_attention_masks.append(encoded_dict['attention_mask'])

# Convert the lists into tensors.
test_input_ids = torch.cat(test_input_ids, dim=0)
test_attention_masks = torch.cat(test_attention_masks, dim=0)
test_sentiments = torch.tensor(test_sentiments)

# Print sentence 0, now as a list of IDs.
print('Original: ', test_tweets[0])
print('Token IDs:', test_input_ids[0])

test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_sentiments)

# CODE AND COMMENTS IN THIS CELL ARE ATTRIBUTABLE TO CHRIS MCCORMICK AND NICK RYAN, NOT THE SECONDARY AUTHOR ([NAME REMOVED])

test_dataloader = DataLoader(
            test_dataset, # The test samples.
            shuffle=True, # shuffle the data
            batch_size = batch_size) # Evaluate with this batch size.

# print(test_input_ids.shape)
# print(test_attention_masks.shape)
# print(test_sentiments.shape)

# CODE AND COMMENTS IN THIS CELL ARE ATTRIBUTABLE TO CHRIS MCCORMICK AND NICK RYAN, NOT THE SECONDARY AUTHOR ([NAME REMOVED])

# Prediction on test set

# Puts model into evaluation mode
model.eval()

total_eval_loss = 0

# Tracking variables 
test_accuracy = 0

# Evaluate data for one epoch
for batch in test_dataloader:
    
    b_labels = batch[2].to(device)
    b_input_ids = batch[0].to(device)
    b_input_mask = batch[1].to(device)
    
    # Tell pytorch not to bother with constructing the compute graph during
    # the forward pass, since this is only needed for backprop (training).
    with torch.no_grad():        

        # Forward pass, calculate logit predictions.
        # token_type_ids is the same as the "segment ids", which 
        # differentiates sentence 1 and 2 in 2-sentence tasks.
        # The documentation for this `model` function is here: 
        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification
        # Get the "logits" output by the model. The "logits" are the output
        # values prior to applying an activation function like the softmax.
        (loss, logits) = model(b_input_ids, 
                               token_type_ids=None, 
                               attention_mask=b_input_mask,
                               labels=b_labels)

    # Accumulate the validation loss.
    total_eval_loss += loss.item()

    # Move logits and labels to CPU
    logits = logits.detach().cpu().numpy()
    label_ids = b_labels.to('cpu').numpy()

    # Calculate the accuracy for this batch of test sentences, and
    # accumulate it over all batches.
    test_accuracy += flat_accuracy(logits, label_ids)

test_accuracy /= len(test_dataloader)
print(test_accuracy)

# This cell is attributable to me ([NAME REMOVED]), although it uses components of the code implemented above
# What it does is compute a probability distribution over the number of classes we're using (well really, it gives us an array of logits corresponding to the different classes, 
# but if you apply a softmax over the logits you'll get the distribution). 
# Here, since we've implemented a binary classification task, we'll get two numbers, P(neg) and P(pos), such that P(neg) + P(pos) = 1. The argmax of this distribution
# is said to be the most likely class, and that is what this function will return.

def predict_sentiment(tweet):
    
    tweet = clean_tweet(tweet)
    tweet_input_id = []
    tweet_attention_mask = []

    tweet_dict = tokenizer.encode_plus(
                            tweet,                      # Sentence to encode.
                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                            max_length = 64,           # Pad & truncate all sentences.
                            truncation=True,           # Explicitly enable truncation
                            pad_to_max_length = True,
                            return_attention_mask = True,   # Construct attn. masks.
                            return_tensors = 'pt',     # Return pytorch tensors.
                       )

    # Add the encoded sentence to the list.    
    tweet_input_id.append(tweet_dict['input_ids'])

    # And its attention mask (simply differentiates padding from non-padding).
    tweet_attention_mask.append(tweet_dict['attention_mask'])

    # Convert the lists into tensors.
    tweet_input_id = torch.cat(tweet_input_id, dim=0)
    tweet_attention_mask = torch.cat(tweet_attention_mask, dim=0)

    tweet_data = TensorDataset(tweet_input_id, tweet_attention_mask)
    
    tweet_dataloader = DataLoader(tweet_data)
    
    for data in tweet_dataloader:
        tweet_input_id = data[0].to(device)
        tweet_attention_mask = data[1].to(device)
    
    # An array of logits corresponding to the probabilities for each class
    tweet_logits = model(tweet_input_id, token_type_ids=None, attention_mask=tweet_attention_mask)
    
    # Puts the logits array on the CPU and turns it into a numpy array
    tweet_logits = tweet_logits[0].detach().cpu().numpy()
    
    # Gives us the index of our predicted class
    prediction = np.argmax(tweet_logits)
    
    # Matches the predicted index with its corresponding class
    if prediction == 1:
        class_ = 'positive'
    else:
        class_ = 'negative'
    
    return "Tweet: " + tweet + "; " + "Predicted Class: " + class_

# A bunch of test examples

print(predict_sentiment("Odio ma terrible vida")) # "I hate my terrible life"
print(predict_sentiment("Este es un momento maravilloso para estar vivo.")) # "This is a wonderful time to be alive"
print(predict_sentiment("Este día no podría ser peor")) # "This day couldn't get any worse"
print(predict_sentiment("¡Qué gran noche!")) # "What a night!"
print(predict_sentiment("Me encanta sentarme en tres horas de tráfico por la mañana.")) # "I love sitting in three hours of traffic in the morning"
print(predict_sentiment("Los baños de burbujas son las mejores cosas de la historia")) # "Bubble baths are the best things ever"
print(predict_sentiment("Me gustaría visitar España alguna vez")) # "I'd like to visit Spain sometime"
print(predict_sentiment("Esto está resultando ser un desastre")) # "This is turning out to be a disaster"
print(predict_sentiment("Es una relación de amor-odio")) # "It's a love-hate relationship"
print(predict_sentiment("Esta es una oración neutral")) # "This is a neutral sentence"